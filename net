2.1 相关工作
一直以来，深度学习领域所取得的诸多成就往往都离不开数据集的贡献。数据集一般会被分为三类：训练集、验证集和测试集。这三类数据集的作用分别是：
1)	训练集用于训练模型。
2)	验证集用于调整模型的超参数和选出最优的模型。
3)	测试集用于评估最终模型的泛化能力。
然而人脸识别在实际使用时通常会遇到不包含在训练集中的新身份，这是人脸识别有别于一般深度学习任务最大不同点之一。例如分类任务在实际使用时也只能用于分类出训练集所包含的类别。于是分类任务只需要关注模型在训练集包含的类别上的分类能力，而人脸识别任务更需要关注模型在新身份上的识别能力。由于验证集一般都是从训练集中切分出来且并不包含新的身份，因此在人脸识别的研究中，研究人员基本都只采用训练集和测试集，不会采用验证集，让测试集兼顾验证集的功能，并且使用不包含在训练集中的新身份所对应的面部图片构成测试集。下面将分别概述人脸识别领域中训练集和测试集的相关工作。
2.1.1 训练集
在研究人员开始将能够支持大规模数据集的深度学习方案应用于人脸识别之后，大规模的人脸数据集开始出现。例如Facebook的Yaniv Taigman等人[1_D]构建了一个拥有四百多万张面部图片的数据集(SCF)，其中有包含4000多个不同的身份。在SCF的助力下，Yaniv Taigman等人训练出了一个接近人类表现的人脸识别模型，充分体现了构建大规模人脸数据集的必要性，不过该数据集没有被开源。
之后首先开源大规模人脸数据集的是Dong Yi等人[4_L]，他们构建了一个拥有大约五十万张面部图片的数据集(CASIA WebFace)，其中有包含大约10000个不同的身份。虽然该数据集的面部图片数量比SCF少很多，但是包含的身份数量是SCF的2倍多，这一点使得Dong Yi等人在该数据集上训练出了一个性能和SCF上相当的模型，且模型的设计结构差距不大。从此处可以发现人脸数据集的大不止要体现在面部图片的数量上，还体现在包含的身份数量上。
Dong Yi等人最大的贡献是不仅将CASIAWebFace开源，而且在其论文中详细介绍了一套从互联网中收集大规模人脸数据集的方法[4_L]：先从IMDb网站上抓取一些感兴趣的名人的名字，然后下载他们页面中的照片。其中网页的主图片几乎都是只有一张本人的脸，但是其他的图片可能包含别人的人脸，于是就以主图片上的人脸为种子，通过一套简单快速的聚类方法注释出其他图片中所需的人脸。最后将测试集包含的身份所对应的面部图片去掉。
上述方案提出的从互联网收集人脸数据集方法催生出了很多新的更好的大规模人脸数据集，例如：
	MS-Celeb-1M[]:总共大约一千万张面部图片，其中包含大约100000个不同的身份
	VGGFace2[]：总共约三百三十一万张面部图片，其中包含9131个不同的身份。
2.1.2 测试集


这些数据集包含了丰富的年龄变化，姿态变化，面部表情变化，面部装饰变化和面部遮挡变化。 
如今，身份识别已经参与到我们生活的方方面面。传统的身份识别方式(ID 卡、个人证件、门禁口令、银行卡账号密码等)都存在很大的危机：以账号密码作为个人身份认证方式的，很难保证账号密码不会被遗忘、丢失、盗取等；以证件作为个人身份认证方式的，很难保证证件不会被伪造；以口令作为个人身份认证方式的，很难保证口令不会被窃取。在这种情况下，更稳定可靠的生物识别技术开始得到快速发展。
生物识别技术[1-3]是指通过对人体生理特征的验证来实现个人身份准确识别的技术。由于人的生理特征与生俱来且各不相同，正好可以作为个人身份确认的识别方式，常用的生物识别方式有：指纹识别[4]，掌纹识别[5-6]，人脸识别，ECG 识别，虹膜识别，DNA 识别等。

本课题在观察从人脸训练集中随机抽样的一部分人脸图片时发现了一个现象：这些来自互联网的名人照片中有很大一部分拥有较好的光照、人脸角度和人脸清晰度，人脸质量较差的图片(既困难样本)只占少数部分。该现象发生的主要原因是：被公开到互联网的图片很多都是由专业人士采用专业相机拍摄的。这种现象将使得人脸识别模型无法在困难样本上得到充分的学习，导致人脸识别模型在无约束场景中不能很稳健地应对质量较差的人脸。
为了尝试解决这个问题，本课题引入了在线困难样本挖掘的方案。截止目前已经有很多实现方案，举例如下：
1)	focal loss：通过模型预测的概率pt，使用(1-Pt)来代表样本难分程度。可以理解为模型对某个样本预测属于其真实label的概率越高，则说明该样本对此模型比较容易学习，反之则难分。 
2)	《ScreenerNet: Learning Self-Paced Curriculum for Deep Neural Networks》论文提出一个附加网络来帮助主网络区分样本难易程度。
3)	《Fine-tuning Convolutional Neural Networks for Biomedical Image Analysis》论文通过对一张图像进行数据增强生成多张图像，然后使用模型预测每张图像的概率。根据多张相同label的增强图像的概率分布区分其样本难易程度。
4)	《OHEM: Training Region-based Object Detectors with Online Hard Example Mining》论文提出先使用模型输出概率，据此选出部分难分样本，然后根据这些样本，更新网络参数。
本课题最终选择参考最为简单巧妙的focal loss来实现在线困难样本挖掘。focal loss是通过改变交叉熵损失来实现的。叉熵损失属于softmax交叉熵损失的一部分，因此可以很方便地和arcface进行结合，其在二元分类场景中的公式为：
公式
在上面，y∈{±1}指定了地面实例类，p∈[0,1]是模型的标签y = 1的类的估计概率，有softmax(本课题中是arcface)计算得来。为了符号方便，我们定义了pt：
公式
这种损失的一个值得注意的特性：即使是容易分类的例子（p >0.5）也会带来较大的损失。这样将很容易掩盖困难样本。于是Tsung-Yi Lin等人在交叉熵损失中添加调制因子（1-pt）γ，可调聚焦参数γ≥0，此时公式将变为：
公式
其中α∈[0,1]加权因子，可以平衡了二分类中正负样本的重要性
我们注意到焦点损失的两个特性：
（1）当一个例子被错误分类并且pt很小时，调制因子接近1并且损失不受影响。 当pt→1时，因子变为0，并且对于良好分类的示例的损失是向下加权的。 
（2）聚焦参数γ平滑地调整简易示例被加权的速率。 当γ= 0时，FL是等效的。
不过Tsung-Yi Lin等人的研究场景是二分类问题，不能直接用于类似多分类问题的人脸识别训练中，于是本课题在保证focal loss形式不变的情况下进行了相应的改进，最后的公式为:
公式
Youyi 等
[32]
利用小波神经网络来构造人脸
识别系统，该方法将小波变换的多分辨特性引入到神经网络之中，很好的克服了较大噪
声下人脸识别困难的问题。史文樊
[33]
通过 Gabor 小波、PCA 算法、粗糙集等对 BP 神经
网络进行了一系列改进，最终提出粗糙集 BP 神经网络算法，该算法不仅提高了人脸识
别精度还对背景、表情等具有很好的鲁棒性。该类方法普遍具有较高的识别精度，是目
前性价比最高的一种方法，但该类方法在识别前需要进行网络训练，当网络权值取值不
当时，网络训练容易陷入局部最优，影响识别效果。
基于深度学习的人脸识别算法研究
4.1 基于MTCNN实现人脸检测和人脸对齐
在人脸识别系统中我们需要人脸检测获取人脸图片，并根据人脸对齐获取的人脸标记点来使用仿射变换进行面部对正。不过在无约束环境中进行人脸检测和对齐极具挑战，各种人脸姿势、复杂的光照条件和各异的面部遮挡都让传统方案基本失效，不过最近的研究表明，深度学习可以在这两项任务上达到不错的效果。本课题将选择多任务级联深度卷积神经网络(MTCNN)来实现这两个任务，由于人脸检测不是本课题的主要工作，所以只是介绍并复现MTCNN。
4.1.1 相关工作
在人脸检测任务中，Viola等人[1]提出的级联人脸检测器利用Haar-Like特征和AdaBoost训练级联分类器，达到了实时的性能。不过有很多证明[2][3][4]表明，即便使用更先进的特征和分类器，这种探测器在现实世界的应用中也会显着降低。随着深度学习在计算机视觉中的不断取得重大突破，Yang等人[5]和Li等人[6]尝试使用深度卷积网络或者级联卷积网络来实现。
在人脸对齐任务中，大致可以分为两种研究方向：基于回归的方法[7][8] [9]和基于模板拟合的方法[10] [11][12]。
不过上述大多数研究都忽略了人脸检测和对齐之间存在固有的相关性，将两个任务结合将带来彼此性能的提升。Kaipeng Zhang等人[13]通过多任务学习和使用级联CNN来集成这两个任务，具体流程如图1所示，详细描述如下：
阶段1：我们利用称为提议网络(P-Net)的完全卷积网络来获得候选面部窗口及其边界框回归向量。然后基于估计的边界框回归向量校准候选者之后，我们采用非最大抑制(NMS)来合并高度重叠的候选者。
阶段2：所有候选人都被送到另一个CNN，称为精炼网络(R-Net)，它进一步拒绝大量错误候选者，使用边界框回归执行校准，并进行NMS。
阶段3：这个阶段类似于第二阶段，但在这个阶段，我们的目标是识别更多监督的面部区域。特别是，网络将输出五个人脸标记点的位置。
图1
4.1.2 在TensorFlow平台上实现MTCNN
本课题主要借鉴了MTCNN作者发布的基于caffe平台的开源代码和其论文中描述的训练细节[]。下面从MTCNN的网络结构，损失函数和数据集这个三要素来介绍实现方案，并给出测试结果。
4.1.2.1 网络结构
本课题基于论文中给出的如图2[]所示的网络结构构建了tensorflow的训练网络。
图二 P-Net、R-Net和O-Net网络结构示意图
4.1.2.2 损失函数
MTCNN的多任务训练需要实现以下损失函数：
1)	面部分类：学习目标被制定为两类分类问题。对于每个样本xi，我们使用交叉熵损失。
 
其中 是网络产生的度量样本 是否是一个人脸的概率。符号 表示真实标签。
2)	边界框回归：我们预测每个候选框与最近的真实人脸框之间的偏移。学习目标被表述为回归问题，我们对每个样本 使用欧几里德损失：
 
	其中 是从网络获得的回归目标， 是地面实况坐标。有四个坐标：左上角横纵坐标，高度和宽度。
3)	面部标记定位：类似于边界框回归任务，面部标记检测被公式化为回归问题，并且同样使用欧几里德损失：
 
其中 是从网络获得的面部标记坐标， 是第i个样本的真实面部标记坐标。有五个面部标:记坐标：左眼，右眼，鼻子，左嘴角和右嘴角。
4)	多源学习：由于我们在每个CNN中使用不同的任务，因此在学习过程中存在不同类型的训练图像，这个时候需要有选择性的组合部分损失函数，其总体损失函数为：
 
	其中N是训练样本的数量， 表示任务重要性。我们在P-Net和R-Net中使用( , , )，而在输出网络(O-Net)中使用( , , )以获得更准确的面部标记。  是样本类型标签。
5)	在线硬样本挖掘：在每个小批量中，对来自所有样本的前向传播中计算的损失进行排序，并选择其中前70％作为硬样本。然后，仅在向后传播中计算来自这些硬样本的梯度。这意味着在训练期间忽略了不太有助于加强探测器的简单样本。实验表明，该策略无需手动选择样本即可获得更好的性能。
4.1.2.3 数据集
训练所需的数据来自WIDER FACE[14]和CelebA[15]，具体使用细节如下：
1)	P-Net：我们从WIDER FACE中随机裁剪几个补丁来收集全部面部，非面部和部分面部。然后，我们将CelebA的面孔裁剪为标记面孔。
2)	R-Net：我们使用框架的第一阶段来检测来自WIDER FACE的面部，以全部面部，非面部和部分面部，同时从CelebA检测到面部标记。
3)	O-Net：与R-Net类似，用于收集数据，但我们使用框架的前两个阶段来检测人脸和收集数据。
4.1.3 测试结果
本课题采用FDDB测试集[MTCNN25]测试MTCNN输出的人脸框，采用AFLW测试集[MTCNN8]测试MTCNN输出的人脸标记点。本课题还测试了MTCNN作者发布的模型，最终测试结果如下表所示：
	FDDB_AP	ALFW MEAN ERROR
		6.9%
	0.763	7.5%
表中的结果显示，本课题复现的MTCNN已经达到MTCNN作者发布的模型性能。虽然MTCNN在两个测试集中的结果不是特别优异，但是考虑到其实时性。
