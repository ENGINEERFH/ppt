
基础网络设计
相关工作
深度学习最初被应用到人脸识别时，人们还只会构建几层神经网络，并且每一层都是根据各自的专家经验手工设计了出来的。例如feacebook。。。
图
不过随着对深度学习研究的深入，有越来越多的证据[41,44]显示神经网络的深度是至关重要的。在权威且具有挑战的ImageNet[36]数据集上主要的优异结果[41,44,13,16]都是采用了较深的神经网络结构。
然而人们很快就发现神经网络是不能通过简单地堆叠来加深网络的，其中主要的障碍就是随着网络深度的增加，梯度消失[1,9]的现象越加严重，这将阻碍网络的收敛。不过之后通过归一化初始化[23,9,37,13]和中间归一化层[16]解决了梯度消失的问题，使得深层网络得以收敛。
可是虽然收敛了，深层网络的训练精度还是随着深度增加而出现退化的问题。针对这一问题，Kaiming He等人[DRL]提出了深度残差学习框架。该框架是通过具有快捷连接[2,34,49]的网络块堆叠而成，如图所示。遵循该框架的网络被称为ResNet。
图
假设期望的底层映射为H(x)，Kaiming He等人通过这样的架构使得每个堆叠的层去适应残差映射F(x):=H(x)-x，而不是直接适应底层映射。这样一来原始的底层映射就可以修改为F(x)+x，该公式就是有图中的快捷连接来实现的。Kaiming He等人在ImageNet上经过全面的实验证明了该框架可以有效的解决深层网络的训练精度退化问题，并得到了比以往网络更好的结果。
在人脸识别领域中，Feng Wang等人[14_N]用ResNet替换掉之前针对人脸识别而精心设计的网络，如图所示
图
该人脸识别网络的主体结构，基本没有变，ResNet加上最后的全连接层被用于从输入的面部图片提取特征，最后在测试集上获得了比之前的网络更好的结果。这说明在人脸识别领域中，有效的层数较深的网络比精心设计的层数较浅的网络更能有效的提取面部图片的特征。特别在无约束的人脸识别。。。。
本课题对人脸识别网络的推理速度和内存占用有明确要求，需要保证在单个NVIDIA TESLA V100 16GB GPU 上能同时推理64张尺寸为112*112像素的面部图片且推理耗时保持在15ms以内。综合推理速度、内存占用和模型精度的考量，本课题采用50层ResNet为基础网络。
残差单元设计
ResNet主体结构是模块化设计，由残差单元(Residual Units)堆叠而成。残差单元有图中所示的两种主要结构。
图
图B被称为瓶颈架构，从原始的两层卷积修改为三层卷积，该三层分别是1×1，3×3和1×1卷积，其中两个1×1卷积分别负责降低维度和恢复维度，使得3×3层工作在较小输入/输出维度。以图为例，图A的参数量为2×3×3×256，图B的参数量为1×1×64+3×3×64+1×1×256，图A的参数量是图B的五倍多，而两者的输入输出维度还是一致，这样的结构可以在损失较少的精度情况下完成模型的精简。受限于有限的算力，Kaiming He等人在50层ResNet及以上的网络中都采用了这样的结构，不过本课题采用的平台提供了丰富的算力，因此我们不需要在50层ResNet的网络中为了精简模型而损失一些精度，于是我们的50层ResNet网络依旧采用了图A的结构。
在ResNet发布之后，Kaiming He等人[IMIDRN]以及人脸识别领域的Jiankang Deng等人[17_A]对ResNet模块提出了各自的修改方案，分别如图1B和图1C所示。
图
	与原始的残差单元相比，最大的不同就是激活层被移到了支路上。这样的修改使得网络更容易训练，泛化能力更强，并在分类任务中取得了更具竞争力的结果。至于图1B和图1C两者的选择中，经过Jiankang Deng等人的证实，在人脸识别领域中，图1C效果优于图1B，于是本课题决定采用图1C作为残差单元的结构。
输入层设计
ResNet在主体结构之前配置了输入模块，如图所示，第一层是卷积层，其步长被设置为2，这样在网络的一开始就提取特征并降低特征图的尺寸，是为了减低计算复杂度，第二层是池化层，可以提升网络的鲁棒性。
图
不过本课题认为该输入模块对人脸识别不是特别友好，因为人脸识别特别注重细节的差异，而步长为2的卷积层和池化层会在输入的一开始就造成面部细节的丢失。于是本课题参考Jiankang Deng等人[17_A]的开源实现，将输入模块修改为如图结构，
图
整体设计
结合地2.1和2,2节中的描述，我们的基础模型设计如图所示。
图
图中的快捷连接分为虚线和实线：虚线表示该快捷连接由当快捷连接由一个1×1的卷积构成，这样是为了应对快捷连接两端的尺寸和维度发生变化的情况；实现表示该快捷连接就是简单的网络连接，可以用在快捷连接两端的尺寸和维度没有发生变化的情况。
添加Swish激活函数
相关工作
在深度学习领域中，ReLU激活函数的提出，显著提升了模型训练时的收敛能力和收敛精度，同时表明了激活函数在深度学习研究中占有重要的席位。可是虽然已经提出了许多激活功能来取代ReLU，但是都没有得到广泛采用，只有一些经过较小改动的变体获得了较多的采用，例如Leaky ReLU和RReLU等。其主要原因是人为设计激活函数的难度较大，于是Google Brain[1_S]采用穷举搜索和强化学习的组合搜索方案来达到激活函数的自动化设计，并成功找到了多个新的激活函数。 
Google Brain主要解决了搜索空间、搜索方案和验收方式这三个关键问题。
搜索空间：为了利用搜索技术，必须设计一个将期望的候选激活函数包含在内的搜索空间。设计搜索空间的一个重要挑战是平衡搜索空间的大小和表现力。过度约束的搜索空间很有可能不包含新颖的激活函数，而过大的搜索空间又难以有效搜索。
Google Brain设计了如图所示的不断重复叠加的拓扑结构，其中灰色背景标识出来的区域就是一个基础核心单元(Core unit)，每个核心包含一元函数(Unary)和二元函数(Binary)：
图
图
通过改变核心单元的数量和搜索算法可选用的一元和二元函数来创建不同的搜索空间。
搜索方案：在穷举搜索的情况下，按照验证准确度获取最佳激活函数的列表。在强化学习的情况下，Google Brain设计了一个如图所示的RNN控制器，其通过强化学习进行训练，目标是最大化验证准确度，其中验证准确度用作奖励。该训练将督促控制器生成具有高验证精度的激活函数。
图
Google Brain为了降低搜索成本，使用20层ResNet作为简单网络架构进行所有搜索，并在CIFAR-10上进行训练。穷举搜索用于小的搜索空间，而RNN控制器用于更大的搜索空间。
验收方式：Google Brain使用受约束的环境进行搜索，这可能会使结果产生偏差，例如性能最佳的激活功能可能仅适用于小型网络。于是Google Brain在获取较优激活函数时，还将这些激活函数推广到更大网络和更大数据集的试验环境中，最终可以发现f(x)=x×sigmoid(β×x)是最佳的激活函数。
Swish添加方案设计
目前Swish总共有两种形式：
	Swish：f(x)=x×sigmoid(x)
	Swish-β：f(x)=x×sigmoid(β×x)
其中Swish-𝛽就是第1.2.1节中提到的激活函数，𝛽是可训练参数。图4是不同𝛽值所对应的曲线，这表明Swish-𝛽可以被视为一个平滑函数，它是线性函数和ReLU函数之间的非线性插值。而Swish函数可以当做是Swish-𝛽在𝛽=1时的特殊情况，且早在Google Brain之前就被Stefan Elfwing[2]等人提出并证实了其优异的效果。所以本课题决定时使用Swish，并且将50层ResNet网络中的所有PRuLU激活函数都替换掉，这样用无参数的Swish替换掉有参数的PReLU函数，可能会带来模型参数减少的同时模型精度提升的期望结果。
图
添加注意力机制
相关工作
从人类自身的角度来讲，注意力机制就是当人们注意到某个目标或某个场景时，该目标内部以及该场景内每一处空间位置上的注意力分布是不一样的。来自人类感知过程的证据[1]显示了注意力机制的重要性，早前人们在神经网络中进行了探索，并取得了很好的效果，例如在分割[2][3][4]和人体姿态估计[5]中提出软注意力结构，该结构是前馈网络结构，通过级联下采样和上采样方式构建。但是在Fei Wang等人[]的研究之前，几乎没有人将注意力机制应用于图像分类中。图像分类中的最新进展基本都是使用具有较多网络层的深度神经网络，其代表之一就是第1.1节中提到的深度残差学习，该方案极大地增加了前馈神经网络的深度。受到注意机制和深度神经网络的最新进展的启发，Fei Wang等人提出了残差注意力网络。
残差注意力网络有如下三个关键技术和其对应的优点：
	堆叠网络结构：残差注意力网络是通过堆叠多个注意力模块构建的。堆叠结构是混合注意力机制的基本应用。因此，能够在不同的注意力模块中捕获不同类型的注意力。
	注意力残差学习：直接堆叠注意力模块会导致明显的性能下降，Fei Wang等人提出了注意力残留学习机制来实现具有数百层的非常深的残差注意力网络，而且随着层越来越深，不同模块的注意力特征会自适应地改变。
	由下采样和上采样级联而成的前馈注意力结构：类似结构已经在人体姿态估计和图像分割任务中得到了成功应用。
堆叠网络结构：一种简单注意力实现方法是使用单个网络分支来生成软权重掩模，可是这种方法在具有挑战性的数据集(如人脸数据集)中使用时就会暴露出如下问题：
	具有杂乱背景，复杂场景和大型外观变化的图像需要通过不同类型的注意力来建模，例如人脸识别。在这种情况下，来自不同层的特征需要由不同的注意掩模建模。使用单个掩码分支将需要指数数量的通道来捕获不同因子的所有组合。
	单个注意力模块仅修改一次特征。如果修改在图像的某些部分失败，则以下网络模块不会获得第二次机会。
残差注意力网络通过堆叠多个注意力模块(Attention Module)来解决上述问题，具体结构如图1所示。
图
前馈注意力结构：2.1节中描述的注意力模块分为两个分支：软掩码分支(Soft Mask Branch)和主干分支(Trunk Branch)，如图2所示。主干分支的功能就是目前所有深成网络的功能，该分支可以是任何先进的深成网络的结构。软掩码分支由下采样和上采样级联成前馈注意力结构，下采样的前馈结构通过多次执行下采样来快速增加感受野，并产生具有强语义信息的低分辨率特征映射，上采样的网络产生密集的特征并与原始特征图相结合。
具体结构如图2中Soft Mask Branch所示，由池化，双线性插值和残差学习单元构成。为了捕获不同比例的信息，还在多次上下采样之前加入由残差学习单元构成的跳跃连接。
图
注意力残差学习：将软掩码分支和主干分支的特征结合有一种非常简单方式：
公式
公式(1)中的T_(i,c) (x)表示主干分支的输出，M_(i,c) (x)表示软掩码分支的输出， i范围是所有空间位置，并且c∈{1,…,C}是通道的索引。这样注意力掩码不仅可以在前向推断期间用作特征选择器，还可以在反向传播期间用作梯度更新的滤波器。公式(1)的梯度可以表示为：
公式
公式(2)中θ是掩码分支参数，∅是主干分支的参数。此属性使注意模块对噪声标签具有鲁棒性。掩码分支可以防止错误的渐变(自嘈杂的标签)新干线参数。但是这样的结合方式运用到2.1中提到的有如下两个问题：
	软掩码分支的输出范围是从0到1，直接用公式(1)将反复降低深层特征的值，使得网络无法正常训练。
	主干网络采用的是性能已经比较优异的深度网络，直接用公式(1)的情况下软掩模分支可能会破坏主干分支的良好性能。
Fei Wang等人参考了ResNet中残差学习的思路，提出了注意力残差学习：
公式
在公式(3)中即使M_(i,c) (x)输出是全0，H_(i,c) (x)也输出也是主干分支的输出F_(i,c) (x)。注意残差学习可以保持原始特征的良好属性，是反向传播能够绕过软掩码分支转发到顶层以削弱掩模分支的过度影响。将注意力残差学习运用到堆叠网络中，随着层数增加，网络可以始终如一地提高性能。
 注意力机制设计
结合第1.3.1节中的内容，本课题选择参考残差注意力网络进行注意力机制的设计，并针对非约束人脸识别场景对残差注意力网络进行了一些改进。选择残差注意力网络的主要原因可以概括三点：
	该网络拥有模块化的精简设计，降低了设计和实现的难度；
	该网络支持深度神经网络，非常适合添加到本课题选择基础网络中；
	该网络的注意力机制不仅能用于选择聚焦位置，还能用于增强该位置处对象的不同表示，正好能适应拥有细致多样的面部细节和复杂多变的背景的非约束人脸识别场景。
主干分支：每一个主干分支，将由两个残差单元构成，且残差单元的结构采用第1.1.2节中选用的残差单元。结构示意图如图右侧所示。
图
软掩膜分支：下采样和上采样的主要网络结构不变，对细节实现做了如下更改：
	其中的残差单元的结构采用第1.1.2节中选用的残差单元。
	Fei Wang等人在软掩码分支最后输出的地方使用了Sigmoid激活函数(如图5所示)，将输出限制在了(0,1)的范围，不过本方案将其更改为Swish激活函数(如图6所示)。原因有两点：a.在x小于0的部分Swish变化比Sigmoid平缓很多，这样可以让生成的背景掩码比较平缓，避免带来更多的背景噪声，而且在x小于0的部分Swish都是负数，这样可以增强去除背景特征的能力；b.在x大于0的部分，Sigmoid在后半阶段的变换是逐渐放缓的，而Swish是基本保持线性变换，这样生成的人脸掩码可以让面部特征的细节得到更大的增强。
图
图
整体结构：结合Fei Wang等人在其论文中给出的残差注意力网络和第1.1节中设计的基础网络结构，本课题设计出了如图所示的用于人脸识别的网络。图为网络的基本结构示意图，展示了残差单元和残差注意力模块的连接细节和每个模块输入输出的特征的尺寸和维度。从图中可以发现输入最后全连接层之前的512维的特征仅仅是经过了3个残差单元，本课题认为3个残差单元在如此高维度低尺寸的场景中是不足以很好地提取特征的。且此处输出的特征的质量是至关重要的，此特征将直接输入到全连接网络，然后转化为用于人脸识别的特征，所以本课题重新设计了如图所示的网络。
图
图
由于图的设计比较合理，且和基础的网络的层数是一直的，

实验
Swish
测试的网络采用Resnet50，主要测试的激活函数是目前应用较广的PReLU和本方案中的Swish。
训练过程采用的超参数和数据集保持一致。在装有8张V100的Atlas上进行训练，总的Batch Size是512，采用含有动量的SGD优化器，动量的超参数为默认的0.9，训练参数的L2正则化权重是默认的5×〖10〗^(-4)，学习率的调整列表是[0.01, 0.001, 0.0001, 0.00001]，调整步骤列表是[45000, 80000, 110000]。
从图5和6中可以发现Swish曲线的震荡幅度是小于PReLU的，表明在训练过程中Swish表现的更加稳定，但是PReLU和Swish的曲线没有明显的上下之分。所以在3K测试集上Swish的精度更高，但是提升不是很明显。不过观察推理速度可以发现，Swish的推理速度是优于PReLU的，而且Swish是无参数的。所以Swish最后带来的效果就是Swish减少模型参数和提高推理速度的同事非但没有降低精度，反而带来了精度的提升。
Attention
本测试一共训练和对比了两个网络：
	Resnet：其中Residual Unit是采用了图4的结构，整个网络的卷积层数是50层
	Attention：采用第3章描述的实现方案，整个网络的主干分支的卷积层数是50层
训练过程采用的超参数和数据集保持一致。在装有8张V100的Atlas上进行训练，总的Batch Size是512，采用含有动量的SGD优化器，动量的超参数为默认的0.9，训练参数的L2正则化权重是默认的5×〖10〗^(-4)，学习率的调整列表是[0.01, 0.001, 0.0001, 0.00001]，调整步骤列表是[45000, 80000, 110000]。
训练结果
在图7中可以观察到一开始Resnet的loss是优于Attention，但是后面就相反，Attention一直保持优势。这一现象表明Attention网络的初始训练难度是要大于Resnet的，此时Attention中软掩码分支产生的错误掩码给网络的训练带来了噪声，但是后面随着软掩码分支渐渐收敛，其产生的正确掩码开始引导网络训练重要的特征，并抑制训练噪声的干扰。图8也表现出同样的现象。
表1展示了这两个模型收敛后在3K华人测试集上的测试结果，Attention在万分之一的指标上有2个百分点的提升，模型精度提升很大。虽然模型的推理速度有所降低，但是推理速度两倍于Resnet50的Resnet100都达不到万分之一上有1个百分点的提升，所以这推理速度降低是可以接受的
由于输入图片尺寸比较小，后面两个Attention模块的可视化效果不明显，所以就只展示第一个Attention模块的可视化效果。图9左边是工卡照的可视化，图9右边是复杂背景且侧脸情况的可视化。
