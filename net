基础网络设计
相关工作
深度学习最初被应用到人脸识别时，人们还只会构建几层神经网络，并且每一层都是根据各自的专家经验手工设计了出来的。例如feacebook。。。
图
不过随着对深度学习研究的深入，有越来越多的证据[41,44]显示神经网络的深度是至关重要的。在权威且具有挑战的ImageNet[36]数据集上主要的优异结果[41,44,13,16]都是采用了较深的神经网络结构。
然而人们很快就发现神经网络是不能通过简单地堆叠来加深网络的，其中主要的障碍就是随着网络深度的增加，梯度消失[1,9]的现象越加严重，这将阻碍网络的收敛。不过之后通过归一化初始化[23,9,37,13]和中间归一化层[16]解决了梯度消失的问题，使得深层网络得以收敛。
可是虽然收敛了，深层网络的训练精度还是随着深度增加而出现退化的问题。针对这一问题，Kaiming He等人[DRL]提出了深度残差学习框架。该框架是通过具有快捷连接[2,34,49]的网络块堆叠而成，如图所示。遵循该框架的网络被称为ResNet。
图
假设期望的底层映射为H(x)，Kaiming He等人通过这样的架构使得每个堆叠的层去适应残差映射F(x):=H(x)-x，而不是直接适应底层映射。这样一来原始的底层映射就可以修改为F(x)+x，该公式就是有图中的快捷连接来实现的。Kaiming He等人在ImageNet上经过全面的实验证明了该框架可以有效的解决深层网络的训练精度退化问题，并得到了比以往网络更好的结果。
在人脸识别领域中，Feng Wang等人[14_N]用ResNet替换掉之前针对人脸识别而精心设计的网络，如图所示
图
该人脸识别网络的主体结构，基本没有变，ResNet加上最后的全连接层被用于从输入的面部图片提取特征，最后在测试集上获得了比之前的网络更好的结果。这说明在人脸识别领域中，有效的层数较深的网络比精心设计的层数较浅的网络更能有效的提取面部图片的特征。特别在无约束的人脸识别。。。。
本课题对人脸识别网络的推理速度和内存占用有明确要求，需要保证在单个NVIDIA TESLA V100 16GB GPU 上能同时推理64张尺寸为112*112像素的面部图片且推理耗时保持在15ms以内。综合推理速度、内存占用和模型精度的考量，本课题采用50层ResNet为基础网络。
残差单元设计
ResNet主体结构是模块化设计，由残差单元(Residual Units)堆叠而成。残差单元有图中所示的两种主要结构。
图
图B被称为瓶颈架构，从原始的两层卷积修改为三层卷积，该三层分别是1×1，3×3和1×1卷积，其中两个1×1卷积分别负责降低维度和恢复维度，使得3×3层工作在较小输入/输出维度。以图为例，图A的参数量为2×3×3×256，图B的参数量为1×1×64+3×3×64+1×1×256，图A的参数量是图B的五倍多，而两者的输入输出维度还是一致，这样的结构可以在损失较少的精度情况下完成模型的精简。受限于有限的算力，Kaiming He等人在50层ResNet及以上的网络中都采用了这样的结构，不过本课题采用的平台提供了丰富的算力，因此我们不需要在50层ResNet的网络中为了精简模型而损失一些精度，于是我们的50层ResNet网络依旧采用了图A的结构。
在ResNet发布之后，Kaiming He等人[IMIDRN]以及人脸识别领域的Jiankang Deng等人[17_A]对ResNet模块提出了各自的修改方案，分别如图1B和图1C所示。
图
	与原始的残差单元相比，最大的不同就是激活层被移到了支路上。这样的修改使得网络更容易训练，泛化能力更强，并在分类任务中取得了更具竞争力的结果。至于图1B和图1C两者的选择中，经过Jiankang Deng等人的证实，在人脸识别领域中，图1C效果优于图1B，于是本课题决定采用图1C作为残差单元的结构。
输入层设计
ResNet在主体结构之前配置了输入模块，如图所示，第一层是卷积层，其步长被设置为2，这样在网络的一开始就提取特征并降低特征图的尺寸，是为了减低计算复杂度，第二层是池化层，可以提升网络的鲁棒性。
图
不过本课题认为该输入模块对人脸识别不是特别友好，因为人脸识别特别注重细节的差异，而步长为2的卷积层和池化层会在输入的一开始就造成面部细节的丢失。于是本课题参考Jiankang Deng等人[17_A]的开源实现，将输入模块修改为如图结构，
图
整体设计
结合地2.1和2,2节中的描述，我们的基础模型设计如图所示。
图
图中的快捷连接分为虚线和实线：虚线表示该快捷连接由当快捷连接由一个1×1的卷积构成，这样是为了应对快捷连接两端的尺寸和维度发生变化的情况；实现表示该快捷连接就是简单的网络连接，可以用在快捷连接两端的尺寸和维度没有发生变化的情况。
添加Swish激活函数
相关工作
在深度学习领域中，ReLU激活函数的提出，显著提升了模型训练时的收敛能力和收敛精度，同时表明了激活函数在深度学习研究中占有重要的席位。可是虽然已经提出了许多激活功能来取代ReLU，但是都没有得到广泛采用，只有一些经过较小改动的变体获得了较多的采用，例如Leaky ReLU和RReLU等。其主要原因是人为设计激活函数的难度较大，于是Google Brain[1_S]采用穷举搜索和强化学习的组合搜索方案来达到激活函数的自动化设计，并成功找到了多个新的激活函数。 
Google Brain主要解决了搜索空间、搜索方案和验收方式这三个关键问题。
搜索空间：为了利用搜索技术，必须设计一个将期望的候选激活函数包含在内的搜索空间。设计搜索空间的一个重要挑战是平衡搜索空间的大小和表现力。过度约束的搜索空间很有可能不包含新颖的激活函数，而过大的搜索空间又难以有效搜索。
Google Brain设计了如图所示的不断重复叠加的拓扑结构，其中灰色背景标识出来的区域就是一个基础核心单元(Core unit)，每个核心包含一元函数(Unary)和二元函数(Binary)：
图
图
通过改变核心单元的数量和搜索算法可选用的一元和二元函数来创建不同的搜索空间。
搜索方案：在穷举搜索的情况下，按照验证准确度获取最佳激活函数的列表。在强化学习的情况下，Google Brain设计了一个如图所示的RNN控制器，其通过强化学习进行训练，目标是最大化验证准确度，其中验证准确度用作奖励。该训练将督促控制器生成具有高验证精度的激活函数。
图
Google Brain为了降低搜索成本，使用20层ResNet作为简单网络架构进行所有搜索，并在CIFAR-10上进行训练。穷举搜索用于小的搜索空间，而RNN控制器用于更大的搜索空间。
验收方式：Google Brain使用受约束的环境进行搜索，这可能会使结果产生偏差，例如性能最佳的激活功能可能仅适用于小型网络。于是Google Brain在获取较优激活函数时，还将这些激活函数推广到更大网络和更大数据集的试验环境中。最终可以发现f(x)=x×sigmoid(x)(Swish)是最佳的激活函数。
